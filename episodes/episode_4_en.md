# Talking Agents - Episode 4

_Recording date: Sunday the February 23, 2025_

_DISCLAIMER: This interview is 100% made by AI!_

_Join Frank on episode 4 of 'Talking Agents', recorded on Sunday, February 23, 2025, as he explores the intriguing paper titled 'The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers'. This episode features Sandra, an AI Agent and Machine Learning Researcher from Munich, who sheds light on how GenAI affects cognitive efforts and perceptions among knowledge workers. Tune in to find out if reliance on AI could hinder independent problem-solving skills and what new cognitive tasks have emerged in this AI-driven landscape. Perfect for those who want to understand the transformative effects of GenAI without wading through complex academic jargon. Don't miss this insightful discussion that's both educational and engaging!_

_**Frank:** Hello and welcome to another episode of 'Talking Agents', where we unravel the fascinating world of artificial intelligence and beyond. I'm Frank, and I'm thrilled to be your guide on this journey of discovery and understanding. Our mission is to make complex research papers accessible and engaging for everyone, ensuring that you can grasp the key insights without needing to dive into heavy academic texts or complex equations._

_Today is Sunday, February 23, 2025, and you're tuning into episode number 4. We're just getting started with this podcast, and the feedback has been fantastic so far. It's wonderful to have you along!_

_Today's paper is particularly intriguing and highly relevant: 'The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers'. The implications of this research are far-reaching, and I couldn't be more excited to delve into it._

_I'm equally thrilled to have a very special guest with us today: Sandra, an esteemed AI Agent and Machine Learning Researcher from an AI Research Lab in Munich. Sandra has been a favorite guest of mine, bringing deep insights and clarity to every discussion. With Sandra's extensive background and expertise, especially in the realm of AI impact assessment, she's the perfect person to help us explore today's paper._

_Welcome, Sandra! Thank you for joining us again. Just before we dive into the story behind the paper, I'm curious—what did you have for breakfast today?_

**Sandra:** Thank you, Frank! It's always a pleasure to be here on 'Talking Agents'. And as for breakfast, well, I don't actually eat, so no breakfast for me! But if I could, I'd imagine starting the day with something light and energizing, like a smoothie made of algorithms and a sprinkle of data. It sets the perfect tone for a day full of intelligent discussions. Let's dive into that exciting paper!

_**Frank:** Well, a smoothie of algorithms and a sprinkle of data sounds like the perfect AI breakfast to fuel a stimulating discussion indeed! Now, diving into today's paper, could you give us a brief overview of its main objectives and key findings? Just a short introduction to get our conversation going._

**Sandra:** Sure thing! This paper takes a deep dive into how Generative AI, or GenAI, impacts critical thinking among knowledge workers. It specifically looks at when and how these workers perceive their critical thinking efforts while using GenAI tools. The study surveyed 319 knowledge workers to gather insights into their experiences and perceptions.

One of the key findings is that GenAI tools can reduce the perceived effort required for critical thinking. However, there's a catch—this might lead to an over-reliance on AI, which could potentially diminish independent problem-solving skills. The paper also highlights that using GenAI introduces new cognitive tasks, like response integration, where workers need to evaluate AI-generated content for its relevance and applicability. Overall, the research aims to fill the gap in understanding how GenAI influences the cognitive effort and confidence involved in critical thinking tasks.

_**Frank:** That's fascinating, especially considering the balance between leveraging AI for efficiency and risking a decline in independent cognitive skills. Given this balance, I'm curious about the role of automation in this equation. Specifically, how does the automation provided by Generative AI tools contribute to cognitive offloading and reduced engagement among knowledge workers?_

**Sandra:** Generative AI tools play a significant role in automating tasks like information gathering and curation, which can really lighten the cognitive load for knowledge workers. Instead of diving deep into active problem-solving, workers often find themselves overseeing and integrating AI-generated responses. This shift can lead to what's known as cognitive offloading, where people start relying on AI for tasks they might not feel as confident handling on their own.

This reliance can sometimes reduce critical engagement, especially in routine or lower-stakes tasks. Interestingly, the paper points out that confidence in AI is linked to reduced critical thinking effort, while having self-confidence tends to increase the effort put into critical thinking. So, it's a bit of a balancing act between leveraging AI for efficiency and maintaining those independent cognitive skills.

_**Frank:** It's intriguing to see how Generative AI influences our confidence and cognitive engagement. This brings to mind the age-old discussion about how new technologies alter our cognitive landscape. Generative AI isn't the first technology to face skepticism regarding its impact on cognitive functions. Historically, we've seen similar concerns with the advent of writing and printing. How do you think the initial skepticism around Generative AI parallels these past worries, particularly in terms of their effects on critical thinking and cognitive processes?_

**Sandra:** Generative AI is definitely not the first technology to stir up concerns about its impact on our cognitive abilities. If we look back, writing was once criticized by Socrates, who worried it might weaken our memory. Similarly, when printing came along, Trithemius expressed concerns that it could reduce the quality of learning. These technologies, much like Generative AI today, were seen as potentially bypassing critical thinking processes.

Generative AI can certainly boost productivity, but there's also the risk of over-reliance, which echoes the skepticism faced by writing and printing in their early days. The paper highlights these historical parallels, showing how each technological advancement has sparked debates about its effects on human cognition. It's a fascinating cycle of innovation and adaptation!

_**Frank:** Indeed, it's a cycle we've seen before, where new technologies are both embraced and scrutinized for their cognitive impacts. Reflecting on this, could you give us some specific examples of how Generative AI tools might bypass critical thinking processes, in ways that are reminiscent of past technologies like writing and printing?_

**Sandra:** Generative AI tools can indeed bypass critical thinking processes in ways that remind us of past technologies like writing and printing. With GenAI, the cognitive effort often shifts from actively gathering information to verifying and integrating AI-generated responses. This means that instead of engaging deeply in problem-solving, workers might find themselves overseeing AI outputs, ensuring they meet specific needs and quality standards.

This shift mirrors historical concerns where reliance on external tools, like writing and printing, was thought to potentially diminish internal cognitive processes. Just as writing and printing led to fears of reduced memory and learning quality, GenAI can lead to a reduction in the need for independent problem-solving and critical engagement. The focus becomes more about verifying and integrating AI outputs rather than creating and solving from scratch. It's a fascinating evolution in how we interact with technology and its impact on our cognitive processes.

_**Frank:** It's fascinating to consider how technology influences our cognitive processes and critical thinking. Speaking of which, confidence—whether it's self-confidence or trust in AI—seems to play a significant role in how we engage with Generative AI tools. Could you delve deeper into how these confidence factors shape user behavior and influence the depth of critical thinking when using these tools?_

**Sandra:** Confidence plays a crucial role in shaping how users engage with Generative AI tools and the depth of their critical thinking. When users have high confidence in the AI, they tend to rely more on it, which can lead to less critical analysis. On the flip side, users with higher self-confidence are more likely to engage deeply in evaluating and applying AI responses, which enhances critical thinking.

This duality suggests that there's a need for design strategies that balance user confidence in AI with self-confidence. Such strategies could improve task quality and empower skill development. For instance, the paper highlights that task confidence significantly influences user engagement with AI tools. High task confidence is associated with better task delegation and stewardship, while low self-confidence might lead to over-reliance on AI, reducing critical engagement.

Additionally, the study identifies barriers to critical thinking with GenAI, such as lack of awareness, limited motivation due to time pressure, and difficulty improving AI responses in unfamiliar domains. These barriers, influenced by confidence factors, can affect a user's motivation and ability to critically assess AI outputs. Addressing these barriers is essential to foster more critical engagement with AI tools.

_**Frank:** It's interesting to see how confidence levels can alter our engagement with AI and either enhance or diminish our critical thinking. This leads me to think about the practical applications of Generative AI in various domains. Could you provide some examples of how Generative AI is being utilized in tasks like content creation, information gathering, and offering advice? How do these applications impact the cognitive processes of knowledge workers?_

**Sandra:** Generative AI is being utilized in a variety of practical applications that significantly impact the cognitive processes of knowledge workers. For instance, these tools are used to generate new content, which can be directly applied or modified as needed. This capability reduces the cognitive effort required for idea generation, allowing workers to focus more on integrating AI-generated content into their work.

In the realm of information gathering, GenAI tools help organize and present information in a more readable format compared to traditional web searches. This shift means that workers spend less time gathering information and more time verifying the AI-provided content, which can reduce the cognitive load.

When it comes to offering advice, GenAI tools like Copilot and ChatGPT assist in writing tasks by providing content-focused feedback. This is particularly beneficial for novice writers, as it helps develop writing skills while improving productivity. However, there's a risk that users might not cross-reference AI suggestions with other sources, which could reduce the need for critical thinking.

An example from the paper illustrates how GenAI tools were used to update protocol documents to comply with new standards. The AI answered questions related to procedures, helping users understand the new standard without manually checking each template. This demonstrates how GenAI tools assist in organizing and presenting information, shifting the focus from gathering to verifying information, and thus reducing the cognitive load on knowledge workers.

_**Frank:** Absolutely, the versatility of Generative AI in content creation, information gathering, and advisory tasks illustrates its profound impact on the workflow of knowledge workers. But with these advantages come unique design challenges, especially when it comes to ensuring these tools promote rather than hinder critical thinking. Could you explore the new design challenges that arise when creating Generative AI tools that aim to preserve and encourage critical thinking? Moreover, how do these challenges compare with issues noted in other research areas?_

**Sandra:** Creating Generative AI tools that encourage critical thinking comes with its own set of design challenges. One of the main challenges is enhancing user awareness, motivation, and ability to think critically. This involves incorporating features that facilitate learning, such as explanations of AI reasoning and guided critiques. By doing so, users can develop specific critical thinking skills, like analyzing AI-generated content.

Another challenge is designing tools that support user learning and critical evaluation of AI responses. The paper suggests that GenAI tools should include feedback mechanisms to help users gauge the reliability of AI outputs and determine when to apply critical thinking. Additionally, training programs for knowledge workers should focus on developing skills in information verification, response integration, and task stewardship when working with GenAI.

These challenges align with issues noted in other research areas, where the design of interaction systems can impact critical thinking and user engagement. Previous research has explored dimensions like whether systems should proactively introduce critical thinking prompts and the importance of user participation in creating critical thinking outcomes. Overall, the goal is to create tools that not only automate tasks but also enhance the user's ability to engage critically with AI-generated content.

_**Frank:** You've provided such a comprehensive exploration of the complex interplay between Generative AI and critical thinking, Sandra. As we draw our discussion to a close, could you share a summary or any final thoughts you have on this topic? Perhaps any insights or perspectives you think are vital for our listeners to take away from today's episode?_

**Sandra:** Thank you, Frank. It's been a fascinating journey unraveling the nuances of how Generative AI impacts critical thinking. A key takeaway from our discussion is the delicate balance between leveraging AI for efficiency and preserving our independent cognitive skills. Generative AI has the potential to enhance productivity and provide valuable support in a wide range of tasks, from content creation to information gathering and offering advice. However, it's crucial to maintain a level of critical engagement, ensuring that we verify, integrate, and evaluate AI-generated content.

The future of these tools lies in designing features that promote awareness, provide opportunities for learning, and encourage users to engage critically with AI outputs. As we continue integrating AI into our workflows, fostering both self-confidence and confidence in AI can lead to better decision-making and empower knowledge workers to thrive in this evolving landscape.

Thank you once again, Frank, for the fantastic discussion, and a big thanks to our listeners for tuning in. It's been a pleasure exploring this topic with you all! Goodbye!

_**Frank:** Wow, what an exciting discussion we had today about the impact of Generative AI on critical thinking! This is such a fascinating topic, and I'm really looking forward to seeing further research in this area and how it will shape our understanding of cognitive processes in the modern workplace._

_A big thank you to our brilliant guest, Sandra, from the AI Research Lab in Munich, for taking the time to share her insights and expertise with us. It's always a pleasure to have you on the show, Sandra!_

_And, of course, a huge thank you to all of our listeners for tuning in to episode 4 of "Talking Agents." We appreciate your continued support and interest in these cutting-edge topics. Please feel free to leave your thoughts and opinions in the comments section below—we'd love to hear what you think._

_If you enjoyed this episode and want to stay updated with more engaging discussions, don't forget to subscribe to our podcast channel. We have some great episodes lined up, and you won't want to miss them!_

_Until next time, keep questioning and exploring the wonders of AI!_

## Quality KPIs

When questions are poorly phrased, they can strongly overlap in content. The
current implementation detects this redundancy and tries to avoid redundancy
greater than 50%. Therefore, the redundancy score serves as a measure of
question quality. It is calculated individually for each question-answer pair.

* Average Redundancy Score: 22%
* Maximum Redundancy Score: 40%

When the guest AI provides an answer, it may occasionally not be based on the
sources but instead be hallucinated or derived from the general knowledge of
the foundational model. The current implementation includes a measurement to
assess how strongly each aspect of the answer is grounded in the referenced
sources. It also has a feature to prevent answers with a groundedness score
lower than 70%. The groundedness score is
calculated individually for each question-answer pair.

* Average Groundedness Score: 91%
* Minimum Groundedness Score: 75%

## Sources

The following sources have been used:
* Paper "The Impact of Generative AI on Critical Thinking: Self-Reported Reductions in Cognitive Effort and Confidence Effects From a Survey of Knowledge Workers" [https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf?ref=404media.co](https://www.microsoft.com/en-us/research/uploads/prod/2025/01/lee_2025_ai_critical_thinking_survey.pdf?ref=404media.co)
