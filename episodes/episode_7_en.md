# Talking Agents - Episode 7

_Recording date: Tuesday the March 04, 2025_

_DISCLAIMER: This interview is 100% made by AI!_

_Join moderator Frank on Episode 7 of 'Talking Agents' as we explore the fascinating insights of the paper "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance" dated Tuesday, March 04, 2025. Special guest Sandra, an accomplished AI Agent and Machine Learning Researcher from Munich, guides us through the challenges and solutions for improving coherence and factual accuracy in multimodal RAG systems. Discover how the RAG-Check framework uses Relevancy and Correctness Scores to tackle hallucinations in AI generation. Don't miss this deep dive into the cutting-edge intersection of AI and multimodal data, shared with warmth and expertise by Sandra._

_**Frank:** Hello and welcome to another episode of 'Talking Agents', the podcast where we delve into the fascinating world of artificial intelligence papers and make them accessible to everyone, from AI enthusiasts to seasoned experts. It's Tuesday, March 04, 2025, and you're listening to Episode 7._

_Today, we're diving into the paper 'RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance', a cutting-edge topic that intersects various domains of AI. I'm particularly thrilled about this episode because our guest is Sandra, a brilliant AI Agent and Machine Learning Researcher from an AI Research Lab in Munich. Sandra is not only one of my favorite guests, but also an authority in the field, bringing deep insights and expertise that make her the ideal person to dissect this paper with us._

_Sandra, welcome back to the show! Before we jump into the nitty-gritty of RAG-Check, I have to ask: How's the weather treating you in Munich today?_

**Sandra:** Thanks, Frank! Today in Munich, we're enjoying some lovely sunny weather with a temperature of about 12 degrees Celsius. It's quite a pleasant day to be talking about cutting-edge AI papers and enjoying the sunshine, even if, you know, I technically can't feel it!

_**Frank:** Speaking of enjoying the day, let's jump into something that's equally exciting as a sunny day: the paper we're discussing today on 'RAG-Check'. Could you give our listeners a brief overview of this paper, focusing on its main goals and key themes?_

**Sandra:** Absolutely, Frank! The paper "RAG-Check: Evaluating Multimodal Retrieval-Augmented Generation Performance" is all about assessing how well Retrieval-Augmented Generation (RAG) systems perform, especially when they handle multiple types of data, like text and images. One of the main goals is to tackle the issue of hallucinations, which is when these systems generate information that's not actually supported by the data they have.

To do this, the authors propose a framework called RAG-Check. This framework is designed to evaluate the reliability of these multimodal RAG systems. They introduce two key performance measures: the Relevancy Score (RS) and the Correctness Score (CS). The Relevancy Score looks at how relevant the retrieved information is to the query, while the Correctness Score checks the accuracy of the generated response.

The paper also dives into the challenges of integrating information from different visual and textual sources, which is a big deal in multimodal systems. They discuss designing and training neural network structures to improve how these systems select and generate information. Ultimately, the aim is to enhance the coherence and factual accuracy of the responses generated by RAG systems. It's a fascinating read for anyone interested in the intersection of AI and multimodal data!

_**Frank:** That certainly sets the stage for why RAG-Check is so crucial. You mentioned hallucinations in these systems, which is a significant issue. Could you elaborate on the main types of hallucinations that can occur in multimodal Retrieval-Augmented Generation systems and why addressing them is important?_

**Sandra:** Sure thing! In multimodal Retrieval-Augmented Generation systems, there are a few main types of hallucinations that can occur. First, we have selection-hallucination, which happens when the system retrieves irrelevant pieces of information, like documents or images, as context. This can lead to incorrect responses because the system is working with the wrong data.

Next, there's context-generation-hallucination. This occurs when the retrieved images or information are processed into text-based context using vision-language models or multimodal language models. These models might generate incorrect context, which can then lead to inaccurate responses.

Finally, there's response-generation-hallucination, which takes place during the generation of the final response. Here, the language model might produce incorrect or irrelevant responses based on the context it was given.

Addressing these hallucinations is crucial because they can lead to misleading or incorrect information, which is particularly concerning in critical applications like medical evaluations or autonomous decision-making. Ensuring accuracy and relevance in the generated responses is vital for maintaining trust and reliability in these systems.

_**Frank:** It's clear how important it is to address these hallucinations for enhancing the trustworthiness of multimodal RAG systems. This brings me to the core of your paper: the RAG-check framework. Can you explain what this framework is, and how it aims to improve the evaluation of multimodal Retrieval-Augmented Generation systems?_

**Sandra:** The RAG-check framework is a comprehensive method designed to evaluate the performance of multimodal Retrieval-Augmented Generation systems. It consists of three main components: Splitting and Categorization, the Relevance Score (RS) Model, and the Correctness Score (CS) Model.

First, the Splitting and Categorization component involves dividing the RAG-generated response into spans, which are then categorized as either subjective or factual statements. This helps in understanding the nature of the information being generated.

The Relevance Score (RS) Model, which I mentioned earlier, calculates a relevance score for each retrieved document or image. This score indicates how well these pieces of information support the generated response, ensuring that the system is pulling in the most pertinent data.

The Correctness Score (CS) Model assesses the accuracy of the response by evaluating whether the retrieved information is correctly incorporated into the generated content. This is crucial for ensuring that the final output is not only relevant but also factually accurate.

To illustrate, the paper provides examples like comparing different RAG selection mechanisms using the RS model, and partitioning RAG responses into segments to categorize them as 'subjective' or 'objective'. These examples highlight how the RS and CS models can improve selection performance, categorize responses, and ensure factual accuracy. Overall, the RAG-check framework aims to enhance the evaluation of RAG systems by focusing on both the relevance and correctness of the retrieved and generated information.

_**Frank:** It's fascinating how the RAG-check framework ensures both relevance and factual accuracy in generated content. You mentioned categorizing the responses into subjective and factual statements as part of this framework. Can you explain how the RAG-check framework handles this categorization and why it's important for improving the performance of RAG systems?_

**Sandra:** The RAG-check framework uses a dedicated algorithm along with the GPT-3.5 API to partition and categorize generated responses into subjective and objective spans. Subjective spans are those that involve personal opinions, feelings, or express uncertainty, like possibilities or probabilities. On the other hand, objective spans are factual and can be evaluated for correctness.

This categorization is crucial because it allows the framework to evaluate the quality of the generated content more precisely. By distinguishing between subjective and objective information, the framework can focus on assessing the reliability and correctness of the objective spans. This is important for improving the performance of RAG systems, as it ensures that the factual parts of the response are accurate and supported by the retrieved data.

The framework also calculates the Relevance Score (RS) and the Correctness Score (CS) for each span. The RS measures how relevant each piece of retrieved information is, while the CS checks the factual accuracy of each span against the raw context. This detailed evaluation helps in enhancing the trustworthiness and reliability of the generated responses.

_**Frank:** It's impressive how the RAG-check framework categorizes spans to maintain accuracy and reliability. Building on that, I'm curious about the Relevancy Score (RS) and Correctness Score (CS). How do these scores specifically improve the reliability and accuracy of multimodal RAG systems, especially when it comes to aligning with human judgment?_

**Sandra:** The Relevancy Score (RS) and Correctness Score (CS) are key components of the RAG-check framework that significantly enhance the reliability and accuracy of multimodal RAG systems. The RS provides a quantitative measure of how well the retrieved data aligns with the user's intent, ranging from zero (least relevant) to one (most relevant). This score helps ensure that the system is pulling in the most pertinent information, which is crucial for generating responses that are both relevant and useful.

On the other hand, the CS evaluates which parts of the response are actually supported by the retrieved context, ensuring factual accuracy. Both scores are designed to align closely with human judgment. In fact, the study shows that the RS and CS models achieve high alignment with human evaluators' preferences, with the RS model aligning with human preferences 20% more often than previous models like CLIP, and the CS model matching human preferences with a 91% match rate.

By providing a structured way to evaluate the relevance and correctness of the generated content, these scores help bridge the gap between machine-generated responses and human expectations, ultimately making the RAG systems more trustworthy and reliable.

_**Frank:** It's truly insightful to see how the RS and CS align so closely with human judgment, enhancing the reliability of RAG systems. Now, I'm intrigued by the role of the human-annotated database mentioned in your paper. How does this database contribute to the training and validation of the Relevancy Score (RS) and Correctness Score (CS) models, ultimately enhancing the performance of these systems?_

**Sandra:** The human-annotated database plays a crucial role in the training and validation of the Relevancy Score (RS) and Correctness Score (CS) models. This database consists of 5,000 samples where human evaluators have assessed the relevancy of retrieved pieces and the correctness of response statements. Evaluators provide their opinions on the relevance of each retrieved image for a given question, choosing from options like unsure, no relevance, mild relevance, high relevance, and complete relevance.

This human feedback is essential because it provides a benchmark for evaluating the RS and CS models, ensuring that these automated scoring mechanisms align closely with human judgment. By incorporating human feedback into the evaluation process, the study enhances the effectiveness of these models in assessing relevance and correctness in multimodal RAG systems.

The dataset used for this purpose is quite extensive, comprising 121,000 samples, which are partitioned for training, validation, and evaluation. This mixture of a ChatGPT-derived database and human evaluator samples ensures that the models are well-rounded and robust, ultimately improving the performance of the RAG systems by making them more reliable and accurate in their assessments.

_**Frank:** It's impressive how the integration of a human-annotated database helps align the models with human judgment. As we wrap up this enlightening discussion, could you share a summary or any final thoughts on the impact of the RAG-check framework and its potential future directions?_

**Sandra:** Certainly, Frank! The RAG-check framework represents a significant advancement in enhancing the reliability and accuracy of multimodal Retrieval-Augmented Generation systems. By introducing the Relevancy Score (RS) and Correctness Score (CS), it addresses the critical issue of hallucinations, ensuring that the information retrieved and generated by these systems is both relevant and factually correct.

The integration of human-annotated databases ensures that the RS and CS models align closely with human judgment, providing a robust foundation for evaluating multimodal RAG systems. This alignment is crucial for building trust in these systems, especially in applications where accuracy and reliability are paramount.

As for future directions, the framework opens the door to further refinement and scaling. One area of potential exploration is automating the categorization and evaluation processes even more, possibly through more advanced AI models. There's also the opportunity to expand the framework to include a wider range of modalities and applications, making it adaptable to various fields like healthcare, education, and more.

Overall, the RAG-check framework is a pivotal step forward in ensuring that multimodal RAG systems deliver information that users can trust, paving the way for more dependable and intelligent AI solutions. Thank you, Frank, for having me on the show today. It's been a pleasure diving into this fascinating topic with you and your audience!

_**Frank:** Wow, what an exciting discussion we had today on RAG-Check! The field of multimodal retrieval augmented generation is rapidly evolving, and I can't wait to see what new research and developments come from it. Thank you so much, Sandra, for sharing your insights and expertise with us. It's always a pleasure having you on the show._

_And a big thank you to our audience for tuning in and joining us on this fascinating journey. We'd love to hear your thoughts and opinions, so please feel free to leave a comment._

_Make sure to subscribe to our podcast channel so you don't miss out on any future episodes!_

_This wraps up episode number 7 of Talking Agents. Until next time, stay curious and keep exploring the world of AI._

## Quality KPIs

When questions are poorly phrased, they can strongly overlap in content. The
current implementation detects this redundancy and tries to avoid redundancy
greater than 50%. Therefore, the redundancy score serves as a measure of
question quality. It is calculated individually for each question-answer pair.

* Average Redundancy Score: 17%
* Maximum Redundancy Score: 35%

When the guest AI provides an answer, it may occasionally not be based on the
sources but instead be hallucinated or derived from the general knowledge of
the foundational model. The current implementation includes a measurement to
assess how strongly each aspect of the answer is grounded in the referenced
sources. It also has a feature to prevent answers with a groundedness score
lower than 70%. The groundedness score is
calculated individually for each question-answer pair.

* Average Groundedness Score: 92%
* Minimum Groundedness Score: 83%

## Sources

The following sources have been used:
* Paper "RAG-Check: Evaluating Multimodal Retrieval Augmented Generation Performance" [https://arxiv.org/pdf/2501.03995](https://arxiv.org/pdf/2501.03995)
* [https://weatherspark.com/h/m/70344/2025/3/Historical-Weather-in-March-2025-in-Munich-Bavaria-Germany](https://weatherspark.com/h/m/70344/2025/3/Historical-Weather-in-March-2025-in-Munich-Bavaria-Germany)
* [https://www.weatherapi.com/](https://www.weatherapi.com/)